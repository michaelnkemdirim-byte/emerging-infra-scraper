#!/usr/bin/env python3
"""
Master Scraper - Runs all country scrapers in parallel and combines results
Executes all scraper_*.py files across all country modules concurrently
Combines all CSV outputs into combined_data.csv
"""

import subprocess
import csv
import time
from datetime import datetime
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import defaultdict
import sys
import json
import anthropic

# Configuration
ROOT_DIR = Path(__file__).parent
COUNTRY_MODULES_DIR = ROOT_DIR / "contryModules"
OUTPUT_FILE = ROOT_DIR / "combined_data.csv"
SECRETS_FILE = ROOT_DIR / "secrets.toml"
MAX_CONCURRENT_SCRAPERS = 20  # Run up to 20 scrapers simultaneously

# CSV fieldnames (standard across all scrapers)
CSV_FIELDNAMES = ['country', 'source', 'title', 'date_iso', 'summary', 'url', 'category', 'status']

# Translation settings
TRANSLATION_CONFIG = {
    'BurkinaFaso/faso7_data.csv': {'source_lang': 'french', 'target_lang': 'english'},
    'BurkinaFaso/gouvernement_data.csv': {'source_lang': 'french', 'target_lang': 'english'},
    'BurkinaFaso/leconomiste_data.csv': {'source_lang': 'french', 'target_lang': 'english'},
    'Ethiopia/mui_data.csv': {'source_lang': 'amharic', 'target_lang': 'english'},
}

# Anthropic translation settings
ANTHROPIC_MODEL = "claude-3-haiku-20240307"  # Most cost-effective for translation
BATCH_SIZE = 25  # Process 25 texts per API call
NUM_TRANSLATION_WORKERS = 1  # Sequential processing to respect rate limits


def discover_scrapers():
    """Discover all scraper files in country modules"""
    scrapers = []

    for country_dir in sorted(COUNTRY_MODULES_DIR.iterdir()):
        if not country_dir.is_dir():
            continue

        for scraper_file in sorted(country_dir.glob("scraper_*.py")):
            # Determine default output filename based on scraper name
            scraper_name = scraper_file.stem.replace('scraper_', '')
            csv_output = country_dir / f"{scraper_name}_data.csv"

            scrapers.append({
                'country': country_dir.name,
                'scraper_file': scraper_file,
                'scraper_name': scraper_name,
                'csv_output': csv_output,
                'working_dir': country_dir
            })

    return scrapers


def cleanup_existing_csv_files(scrapers):
    """Delete all existing CSV files generated by scrapers"""
    print("\n" + "="*80)
    print("CLEANING UP EXISTING CSV FILES")
    print("="*80)

    deleted_count = 0

    # Delete individual scraper CSV files (both original and translated)
    for scraper_info in scrapers:
        csv_file = scraper_info['csv_output']
        en_csv_file = csv_file.parent / csv_file.name.replace('_data.csv', '_data_en.csv')

        # Delete original CSV
        if csv_file.exists():
            try:
                csv_file.unlink()
                print(f"  ✓ Deleted: {scraper_info['country']}/{csv_file.name}")
                deleted_count += 1
            except Exception as e:
                print(f"  ✗ Error deleting {csv_file}: {e}")

        # Delete English translation CSV
        if en_csv_file.exists():
            try:
                en_csv_file.unlink()
                print(f"  ✓ Deleted: {scraper_info['country']}/{en_csv_file.name}")
                deleted_count += 1
            except Exception as e:
                print(f"  ✗ Error deleting {en_csv_file}: {e}")

    # Delete combined CSV file if exists
    if OUTPUT_FILE.exists():
        try:
            OUTPUT_FILE.unlink()
            print(f"  ✓ Deleted: {OUTPUT_FILE.name}")
            deleted_count += 1
        except Exception as e:
            print(f"  ✗ Error deleting {OUTPUT_FILE}: {e}")

    print(f"\n✓ Cleaned up {deleted_count} existing CSV files")
    print("="*80)


def run_scraper(scraper_info):
    """Run a single scraper and return its status"""
    country = scraper_info['country']
    scraper_name = scraper_info['scraper_name']
    scraper_file = scraper_info['scraper_file']
    csv_output = scraper_info['csv_output']
    working_dir = scraper_info['working_dir']

    start_time = time.time()

    try:
        # Run scraper with output file argument
        result = subprocess.run(
            ['python3', scraper_file.name, '--output', csv_output.name],
            cwd=working_dir,
            capture_output=True,
            text=True,
            timeout=600  # 10 minute timeout per scraper
        )

        elapsed = time.time() - start_time

        # Check if CSV was created and count records
        record_count = 0
        if csv_output.exists():
            try:
                with open(csv_output, 'r', encoding='utf-8') as f:
                    record_count = sum(1 for line in f) - 1  # Subtract header
            except:
                record_count = 0

        return {
            'country': country,
            'scraper_name': scraper_name,
            'success': result.returncode == 0,
            'record_count': record_count,
            'elapsed_time': elapsed,
            'csv_file': csv_output if csv_output.exists() else None,
            'error': None if result.returncode == 0 else result.stderr[:200]
        }

    except subprocess.TimeoutExpired:
        elapsed = time.time() - start_time
        return {
            'country': country,
            'scraper_name': scraper_name,
            'success': False,
            'record_count': 0,
            'elapsed_time': elapsed,
            'csv_file': None,
            'error': 'Timeout (>10 minutes)'
        }

    except Exception as e:
        elapsed = time.time() - start_time
        return {
            'country': country,
            'scraper_name': scraper_name,
            'success': False,
            'record_count': 0,
            'elapsed_time': elapsed,
            'csv_file': None,
            'error': str(e)[:200]
        }


def load_anthropic_api_key():
    """Load Anthropic API key from secrets.toml"""
    try:
        import toml
        with open(SECRETS_FILE, 'r') as f:
            secrets = toml.load(f)
            return secrets.get('anthropicAPI')
    except Exception as e:
        print(f"Error loading API key from secrets.toml: {e}")
        return None


def translate_batch_simple(client, texts, source_lang, target_lang):
    """Simplified batch translation for master scraper integration"""
    if not texts:
        return {}

    numbered_texts = "\n".join([f"{i+1}. {t['text']}" for i, t in enumerate(texts)])

    prompt = f"""Translate the following {len(texts)} {source_lang.title()} texts to {target_lang.title()}.
Preserve technical terms, organization names, and proper nouns.
IMPORTANT: Output ONLY a valid JSON array. Escape all special characters properly.

Format: ["translation 1", "translation 2", ...]

Texts to translate:
{numbered_texts}"""

    # Retry for rate limiting
    max_retries = 3
    response = None

    for attempt in range(max_retries):
        try:
            response = client.messages.create(
                model=ANTHROPIC_MODEL,
                max_tokens=4096,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
            )
            break  # Success, exit retry loop

        except Exception as api_error:
            error_str = str(api_error)

            # Handle 429 rate limit errors
            if '429' in error_str or 'rate' in error_str.lower() or 'exceeded' in error_str.lower():
                if attempt < max_retries - 1:
                    print(f"      ⚠️  Rate limit hit, retrying immediately {attempt + 2}/{max_retries}...")
                    continue
                else:
                    print(f"      ❌ Rate limit persists after {max_retries} retries")
                    return {t['id']: t['text'] for t in texts}
            else:
                # Non-rate-limit error
                print(f"      ⚠ Translation error: {error_str[:100]}")
                return {t['id']: t['text'] for t in texts}

    if response is None:
        return {t['id']: t['text'] for t in texts}

    try:
        result_text = response.content[0].text.strip()
        start = result_text.find('[')
        end = result_text.rfind(']') + 1

        if start == -1 or end == 0:
            return {t['id']: t['text'] for t in texts}

        json_str = result_text[start:end]

        try:
            translations = json.loads(json_str, strict=False)
        except json.JSONDecodeError:
            # Clean special characters
            cleaned = json_str
            cleaned = cleaned.replace('\u2018', "'").replace('\u2019', "'")
            cleaned = cleaned.replace('\u201C', '"').replace('\u201D', '"')
            cleaned = cleaned.replace('\u00AB', '"').replace('\u00BB', '"')
            cleaned = cleaned.replace('\u2013', '-').replace('\u2014', '-')
            cleaned = cleaned.replace('\u2026', '...').replace('\u00A0', ' ')
            cleaned = cleaned.replace('\u2022', '*')
            translations = json.loads(cleaned, strict=False)

        result = {}
        for i, text_info in enumerate(texts):
            if i < len(translations):
                result[text_info['id']] = translations[i]
            else:
                result[text_info['id']] = text_info['text']

        return result

    except Exception as e:
        print(f"      ⚠ Translation error: {str(e)[:100]}")
        return {t['id']: t['text'] for t in texts}


def translate_csv_file(api_key, csv_file, source_lang, target_lang):
    """Translate a CSV file to English"""
    if not csv_file.exists():
        return None

    country_name = csv_file.parent.name
    file_name = csv_file.name

    print(f"    Translating {country_name}/{file_name} ({source_lang} → {target_lang})...")

    try:
        with open(csv_file, 'r', encoding='utf-8') as f:
            rows = list(csv.DictReader(f))
    except Exception as e:
        print(f"      ✗ Error reading: {e}")
        return None

    if not rows:
        print(f"      ✓ Empty file, skipping")
        return None

    # Collect texts to translate
    texts_to_translate = []
    translation_map = {}

    for row_idx, row in enumerate(rows):
        for field in ['title', 'summary']:
            if row.get(field):
                text_id = len(texts_to_translate)
                texts_to_translate.append({'id': text_id, 'text': row[field]})
                translation_map[(row_idx, field)] = text_id

    if not texts_to_translate:
        print(f"      ✓ No content to translate")
        return None

    # Translate in batches
    client = anthropic.Anthropic(api_key=api_key)
    all_translations = {}

    for i in range(0, len(texts_to_translate), BATCH_SIZE):
        batch = texts_to_translate[i:i+BATCH_SIZE]
        batch_translations = translate_batch_simple(client, batch, source_lang, target_lang)
        all_translations.update(batch_translations)

    # Apply translations and clean text
    translated_rows = [row.copy() for row in rows]
    for (row_idx, field), text_id in translation_map.items():
        if text_id in all_translations:
            text = all_translations[text_id]
            # Remove newlines and clean whitespace
            text = text.replace('\n', ' ').replace('\r', ' ')
            text = ' '.join(text.split())
            translated_rows[row_idx][field] = text

    # Also clean non-translated text
    for row in translated_rows:
        for field in ['title', 'summary']:
            if row.get(field):
                row[field] = row[field].replace('\n', ' ').replace('\r', ' ')
                row[field] = ' '.join(row[field].split())

    # Write translated CSV
    output_file = csv_file.parent / csv_file.name.replace('_data.csv', '_data_en.csv')

    try:
        with open(output_file, 'w', encoding='utf-8', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=CSV_FIELDNAMES)
            writer.writeheader()
            writer.writerows(translated_rows)

        print(f"      ✓ Created {output_file.name} ({len(texts_to_translate)} texts translated)")
        return output_file

    except Exception as e:
        print(f"      ✗ Error writing: {e}")
        return None


def translate_scraped_files(results):
    """Translate non-English CSV files after scraping"""
    print("\n" + "="*80)
    print("TRANSLATING NON-ENGLISH CSV FILES")
    print("="*80)

    api_key = load_anthropic_api_key()
    if not api_key:
        print("⚠ Anthropic API key not found in credentials.txt - skipping translation")
        return

    # Identify files that need translation
    files_to_translate = []
    for result in results:
        if result['csv_file'] and result['csv_file'].exists():
            csv_path = result['csv_file']
            relative_path = f"{csv_path.parent.name}/{csv_path.name}"

            if relative_path in TRANSLATION_CONFIG:
                config = TRANSLATION_CONFIG[relative_path]
                files_to_translate.append({
                    'csv_file': csv_path,
                    'source_lang': config['source_lang'],
                    'target_lang': config['target_lang']
                })

    if not files_to_translate:
        print("✓ No files need translation (all already in English)")
        return

    print(f"Found {len(files_to_translate)} file(s) to translate\n")

    # Translate each file
    for file_info in files_to_translate:
        translate_csv_file(
            api_key,
            file_info['csv_file'],
            file_info['source_lang'],
            file_info['target_lang']
        )

    print(f"\n✓ Translation complete")
    print("="*80)


def combine_csv_files(results, output_file):
    """Combine all *_en.csv files into a single output file"""
    print("\n" + "="*80)
    print("COMBINING ENGLISH CSV FILES (*_en.csv)")
    print("="*80)

    total_records = 0

    # Look for _en.csv files instead of regular _data.csv files
    csv_files = []
    for r in results:
        if r['csv_file'] and r['csv_file'].exists():
            # Check for _en.csv version first
            en_csv_file = r['csv_file'].parent / r['csv_file'].name.replace('_data.csv', '_data_en.csv')

            if en_csv_file.exists():
                csv_files.append(en_csv_file)
            else:
                # Fallback to original file if no _en version exists
                csv_files.append(r['csv_file'])

    if not csv_files:
        print("No CSV files to combine!")
        return 0

    print(f"Combining {len(csv_files)} CSV files...")

    with open(output_file, 'w', newline='', encoding='utf-8') as outfile:
        writer = csv.DictWriter(outfile, fieldnames=CSV_FIELDNAMES)
        writer.writeheader()

        for csv_file in csv_files:
            try:
                with open(csv_file, 'r', encoding='utf-8') as infile:
                    reader = csv.DictReader(infile)
                    records_written = 0

                    for row in reader:
                        # Ensure all required fields are present
                        filtered_row = {field: row.get(field, '') for field in CSV_FIELDNAMES}
                        writer.writerow(filtered_row)
                        records_written += 1
                        total_records += 1

                    en_marker = " [EN]" if "_en.csv" in csv_file.name else ""
                    print(f"  ✓ {csv_file.parent.name}/{csv_file.name}{en_marker}: {records_written} records")

            except Exception as e:
                print(f"  ✗ Error reading {csv_file}: {e}")

    print(f"\n✓ Combined {total_records} total records into {output_file}")
    return total_records


def print_summary(results, total_elapsed):
    """Print execution summary"""
    print("\n" + "="*80)
    print("MASTER SCRAPER EXECUTION SUMMARY")
    print("="*80)

    successful = [r for r in results if r['success']]
    failed = [r for r in results if not r['success']]

    print(f"\nTotal scrapers: {len(results)}")
    print(f"Successful: {len(successful)} ({len(successful)/len(results)*100:.1f}%)")
    print(f"Failed: {len(failed)} ({len(failed)/len(results)*100:.1f}%)")
    print(f"\nTotal execution time: {total_elapsed:.1f} seconds ({total_elapsed/60:.1f} minutes)")

    # Records by country
    print("\n" + "-"*80)
    print("RECORDS BY COUNTRY")
    print("-"*80)
    country_stats = defaultdict(lambda: {'scrapers': 0, 'records': 0, 'failed': 0})

    for r in results:
        country = r['country']
        country_stats[country]['scrapers'] += 1
        if r['success']:
            country_stats[country]['records'] += r['record_count']
        else:
            country_stats[country]['failed'] += 1

    for country in sorted(country_stats.keys()):
        stats = country_stats[country]
        print(f"{country:20} {stats['scrapers']} scrapers, {stats['records']:5} records, {stats['failed']} failed")

    # Successful scrapers details
    if successful:
        print("\n" + "-"*80)
        print("SUCCESSFUL SCRAPERS")
        print("-"*80)
        print(f"{'Country':<20} {'Scraper':<25} {'Records':<10} {'Time (s)'}")
        print("-"*80)

        for r in sorted(successful, key=lambda x: x['country']):
            print(f"{r['country']:<20} {r['scraper_name']:<25} {r['record_count']:<10} {r['elapsed_time']:.1f}")

    # Failed scrapers
    if failed:
        print("\n" + "-"*80)
        print("FAILED SCRAPERS")
        print("-"*80)
        print(f"{'Country':<20} {'Scraper':<25} {'Error'}")
        print("-"*80)

        for r in failed:
            error = r['error'][:50] if r['error'] else 'Unknown error'
            print(f"{r['country']:<20} {r['scraper_name']:<25} {error}")

    print("\n" + "="*80)


def categorize_batch(client, articles_batch):
    """
    Categorize a batch of articles using Mistral API

    Args:
        client: Mistral API client
        articles_batch: List of dicts with 'title' and 'summary'

    Returns:
        List of category strings
    """
    prompt = f"""You are an infrastructure news classifier. Categorize each article into ONE category.

IMPORTANT: First determine if the article is about infrastructure at all. Many articles may be general news, tributes, corporate announcements, customer service events, etc. that are NOT infrastructure-related.

CATEGORIES (in priority order):
- NonInfra: NOT infrastructure-related (tributes, condolences, customer service events, general corporate news, HR announcements, awards, celebrations, etc.)
- port: Port, maritime, shipping, airport infrastructure projects
- rail: Railway, metro, train infrastructure projects
- highway: Road, highway, bridge infrastructure projects
- SEZ: Special Economic Zones, industrial parks, economic zones
- smart city: Smart city initiatives, digital infrastructure, urban tech
- Infrastructure: Infrastructure-related but doesn't fit the specific categories above (catch-all for infra)

CLASSIFICATION LOGIC:
1. If article is NOT about infrastructure → NonInfra
2. If article is about infrastructure → Try to match to specific category (port/rail/highway/SEZ/smart city)
3. If infrastructure but doesn't match specific categories → Infrastructure

ARTICLES:
"""

    for i, article in enumerate(articles_batch):
        title = article.get('title', '')
        summary = article.get('summary', '')
        prompt += f"\n[{i}] Title: {title}\nSummary: {summary[:300]}\n"

    prompt += f"""
OUTPUT FORMAT:
Return ONLY a valid JSON array with {len(articles_batch)} category strings.
Example: ["port", "highway", "NonInfra", "rail"]

JSON Array:"""

    try:
        response = client.messages.create(
            model=ANTHROPIC_MODEL,
            max_tokens=500,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1
        )

        content = response.content[0].text.strip()

        # Remove markdown code blocks if present
        if content.startswith('```'):
            content = content.split('```')[1]
            if content.startswith('json'):
                content = content[4:]
            content = content.strip()

        results = json.loads(content)

        if not isinstance(results, list) or len(results) != len(articles_batch):
            return ["NonInfra" for _ in articles_batch]

        validated_results = []
        for result in results:
            if isinstance(result, str):
                category = result
            elif isinstance(result, dict):
                category = result.get('category', 'NonInfra')
            else:
                category = "NonInfra"

            if category not in CATEGORIES:
                category = "NonInfra"

            validated_results.append(category)

        return validated_results

    except Exception as e:
        print(f"  ❌ Categorization error: {e}")
        return ["NonInfra" for _ in articles_batch]


def categorize_and_filter_combined_data(api_key):
    """Categorize combined_data.csv and remove NonInfra articles"""
    if not OUTPUT_FILE.exists():
        print("  ⚠️  No combined_data.csv found, skipping categorization")
        return 0

    print(f"\n📊 Categorizing {OUTPUT_FILE.name}...")

    client = anthropic.Anthropic(api_key=api_key)

    # Read CSV
    with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
        rows = list(csv.DictReader(f))

    total_rows = len(rows)
    print(f"  Total articles: {total_rows}")

    # Sources with VERIFIED CORRECT pre-categorization (infrastructure-specific sites)
    # These should be PRESERVED and NOT re-categorized
    PRESERVE_SOURCES = [
        'Engineering News',
        'Infrastructure News',
        'Construction Kenya',
        'Nairametrics',
        'Federal Roads Maintenance Agency (FERMA)',
        'Nigerian Ports Authority'
    ]

    # Only categorize general news sources (which have incorrect WordPress API categories)
    rows_to_categorize = [
        (i, row) for i, row in enumerate(rows)
        if row.get('source') not in PRESERVE_SOURCES
    ]

    preserved_count = total_rows - len(rows_to_categorize)
    if preserved_count > 0:
        print(f"  ⏭️  Preserved {preserved_count} articles from specialized infrastructure sources")

    print(f"  Categorizing {len(rows_to_categorize)} articles (General news sources only)...")

    categorized_count = 0

    for batch_start in range(0, len(rows_to_categorize), CATEGORIZATION_BATCH_SIZE):
        batch_end = min(batch_start + CATEGORIZATION_BATCH_SIZE, len(rows_to_categorize))
        batch = rows_to_categorize[batch_start:batch_end]

        articles_batch = [row for _, row in batch]

        print(f"  Batch {batch_start//CATEGORIZATION_BATCH_SIZE + 1}/{(len(rows_to_categorize)-1)//CATEGORIZATION_BATCH_SIZE + 1}...", end=' ')
        results = categorize_batch(client, articles_batch)

        for (row_idx, row), category in zip(batch, results):
            rows[row_idx]['category'] = category
            categorized_count += 1

        print(f"✓ ({categorized_count}/{len(rows_to_categorize)})")

    # Filter out NonInfra rows
    original_count = len(rows)
    rows = [row for row in rows if row.get('category') != 'NonInfra']
    removed_count = original_count - len(rows)

    print(f"  🗑️  Removed {removed_count} NonInfra articles")
    print(f"  ✅ Kept {len(rows)} infrastructure articles")

    # Write filtered CSV
    with open(OUTPUT_FILE, 'w', encoding='utf-8', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=CSV_FIELDNAMES)
        writer.writeheader()
        writer.writerows(rows)

    # Print category breakdown
    category_counts = {}
    for row in rows:
        cat = row.get('category', 'unknown')
        category_counts[cat] = category_counts.get(cat, 0) + 1

    print(f"  📊 Category breakdown:")
    for cat in CATEGORIES:
        if cat != 'NonInfra':  # Skip NonInfra since we filtered it out
            count = category_counts.get(cat, 0)
            if count > 0:
                print(f"     - {cat}: {count}")

    return len(rows)


def main():
    """Main execution function"""
    print("="*80)
    print("MASTER SCRAPER - INFRASTRUCTURE DATA COLLECTION")
    print("="*80)
    print(f"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"Max concurrent scrapers: {MAX_CONCURRENT_SCRAPERS}")
    print("="*80)

    start_time = time.time()

    # Discover all scrapers
    print("\nDiscovering scrapers...")
    scrapers = discover_scrapers()
    print(f"Found {len(scrapers)} scrapers across {len(set(s['country'] for s in scrapers))} countries")

    # Clean up existing CSV files
    cleanup_existing_csv_files(scrapers)

    # Load Anthropic API key once for all translations
    api_key = load_anthropic_api_key()
    if not api_key:
        print("\n⚠ Anthropic API key not found - translation will be skipped")

    # Run all scrapers in parallel
    print(f"\nRunning {len(scrapers)} scrapers with max {MAX_CONCURRENT_SCRAPERS} concurrent...")
    print("-"*80)

    results = []
    completed_count = 0

    # Track which non-English CSVs need translation
    files_to_translate = []
    non_english_scrapers_total = len([s for s in scrapers
                                      if f"{s['country']}/{s['scraper_name']}_data.csv" in TRANSLATION_CONFIG])
    non_english_completed = 0
    translation_started = False
    translation_executor = ThreadPoolExecutor(max_workers=1)
    translation_future = None

    def do_translations(files_list, api_key):
        """Run all translations sequentially in a separate thread"""
        print("\n" + "="*80)
        print(f"ALL NON-ENGLISH SCRAPERS COMPLETE - STARTING TRANSLATION")
        print("="*80)

        # Translate sequentially (one file at a time)
        for file_info in files_list:
            print(f"\nTranslating {file_info['relative_path']} ({file_info['source_lang']} → {file_info['target_lang']})...")
            translate_csv_file(
                api_key,
                file_info['csv_file'],
                file_info['source_lang'],
                file_info['target_lang']
            )

        print("\n✓ All translations complete")
        print("="*80 + "\n")

    with ThreadPoolExecutor(max_workers=MAX_CONCURRENT_SCRAPERS) as scraper_executor:
        future_to_scraper = {scraper_executor.submit(run_scraper, scraper): scraper for scraper in scrapers}

        for future in as_completed(future_to_scraper):
            result = future.result()
            results.append(result)
            completed_count += 1

            status_icon = "✓" if result['success'] else "✗"
            print(f"[{completed_count}/{len(scrapers)}] {status_icon} {result['country']:20} {result['scraper_name']:25} "
                  f"{result['record_count']:5} records in {result['elapsed_time']:.1f}s")

            # Check if this scraper needs translation
            if result['success'] and result['csv_file'] and result['csv_file'].exists():
                csv_path = result['csv_file']
                relative_path = f"{csv_path.parent.name}/{csv_path.name}"

                if relative_path in TRANSLATION_CONFIG:
                    config = TRANSLATION_CONFIG[relative_path]
                    files_to_translate.append({
                        'csv_file': csv_path,
                        'source_lang': config['source_lang'],
                        'target_lang': config['target_lang'],
                        'relative_path': relative_path
                    })
                    non_english_completed += 1
                    print(f"    → Queued for translation ({non_english_completed}/{non_english_scrapers_total})")

                    # Start translation thread as soon as ALL non-English scrapers complete
                    if non_english_completed == non_english_scrapers_total and not translation_started and api_key:
                        translation_started = True
                        # Launch translation in separate thread (doesn't block remaining scrapers)
                        translation_future = translation_executor.submit(do_translations, files_to_translate.copy(), api_key)

    # Wait for translation to complete if it was started
    if translation_future:
        translation_future.result()

    translation_executor.shutdown(wait=True)

    # Combine all CSV files
    total_records = combine_csv_files(results, OUTPUT_FILE)

    # Categorize and filter combined data using categorize_data.py
    if api_key and total_records > 0:
        print("\n" + "="*80)
        print("CATEGORIZATION AND FILTERING")
        print("="*80)

        # Run categorize_data.py script
        import subprocess
        categorize_script = Path(__file__).parent / "categorize_data.py"
        result = subprocess.run(
            ["python", str(categorize_script)],
            capture_output=False,
            text=True
        )

        if result.returncode == 0:
            # Count final records after categorization
            if OUTPUT_FILE.exists():
                with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
                    final_records = sum(1 for _ in csv.DictReader(f))
            else:
                final_records = 0
        else:
            print(f"  ⚠️  Categorization script failed with exit code {result.returncode}")
            final_records = total_records

        print("="*80)
    else:
        if not api_key:
            print("\n⚠️  Skipping categorization (no API key)")
        final_records = total_records

    # Print summary
    total_elapsed = time.time() - start_time
    print_summary(results, total_elapsed)

    print(f"\n✓ Combined data saved to: {OUTPUT_FILE}")
    print(f"✓ Total raw records: {total_records}")
    if api_key and final_records != total_records:
        print(f"✓ Infrastructure records (after filtering): {final_records}")
    print(f"✓ Completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*80)

    # Exit with error code if any scrapers failed
    failed_count = len([r for r in results if not r['success']])
    if failed_count > 0:
        print(f"\n⚠ Warning: {failed_count} scraper(s) failed")
        sys.exit(1)
    else:
        print("\n✓ All scrapers completed successfully!")
        sys.exit(0)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\n⚠ Interrupted by user")
        sys.exit(130)
    except Exception as e:
        print(f"\n\n✗ Fatal error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
